# =============================================================================
# Конфигурация LLM-сервиса
# Скопируйте в .env перед запуском: cp env.example .env
# =============================================================================

# -----------------------------------------------------------------------------
# ВЫБОР BACKEND'А (vllm / ollama / llamacpp)
# -----------------------------------------------------------------------------
LLM_BACKEND=vllm

# Порт, на котором будет доступен LLM API (по умолчанию 8000)
LLM_PORT=8000

# Имя модели, которое возвращается в /v1/models и используется в запросах
SERVED_MODEL_NAME=qwen3-8b

# Максимальная длина контекста (токены)
MAX_MODEL_LEN=8192

# -----------------------------------------------------------------------------
# HUGGING FACE
# -----------------------------------------------------------------------------
# Токен HF (нужен для gated/private моделей)
HF_TOKEN=

# Путь к кэшу HF на хосте
HF_CACHE_DIR=~/.cache/huggingface

# -----------------------------------------------------------------------------
# VLLM (профиль: vllm)
# -----------------------------------------------------------------------------
# Модель для vLLM (HF repo id)
VLLM_MODEL=Qwen/Qwen3-8B-AWQ

# Квантизация (awq, gptq, squeezellm, или пусто для fp16/bf16)
VLLM_QUANTIZATION=awq

# Тип KV cache (fp8, auto)
VLLM_KV_CACHE_DTYPE=fp8

# Использование GPU памяти (0.0-1.0)
VLLM_GPU_MEM_UTIL=0.65

# Максимум параллельных запросов
VLLM_MAX_NUM_SEQS=2

# -----------------------------------------------------------------------------
# OLLAMA (профиль: ollama)
# -----------------------------------------------------------------------------
# Имя модели в Ollama (скачивается через ollama pull)
OLLAMA_MODEL=qwen3:8b

# Путь к моделям Ollama на хосте
OLLAMA_MODELS_DIR=./data/ollama

# Параллельные запросы
OLLAMA_NUM_PARALLEL=2

# Максимум загруженных моделей в память
OLLAMA_MAX_LOADED_MODELS=1

# -----------------------------------------------------------------------------
# LLAMA.CPP (профиль: llamacpp)
# -----------------------------------------------------------------------------
# Имя файла модели (GGUF) в папке LLAMACPP_MODELS_DIR
LLAMACPP_MODEL_FILE=qwen3-8b-q4_k_m.gguf

# Путь к папке с GGUF-моделями на хосте
LLAMACPP_MODELS_DIR=./data/llamacpp

# Количество слоёв на GPU (99 = все)
LLAMACPP_GPU_LAYERS=99

# Шаблон чата (qwen3, llama3, chatml, и т.д.)
LLAMACPP_CHAT_TEMPLATE=qwen3

# -----------------------------------------------------------------------------
# EMBEDDING (vLLM, порт 8001)
# -----------------------------------------------------------------------------
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDING_SERVED_NAME=qwen3-embedding-0.6b
EMBEDDING_GPU_MEM_UTIL=0.2
EMBEDDING_MAX_LEN=768

# -----------------------------------------------------------------------------
# RERANKER (vLLM, порт 8002)
# -----------------------------------------------------------------------------
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L6-v2
RERANKER_SERVED_NAME=ms-marco-mini-l6-v2
RERANKER_GPU_MEM_UTIL=0.1
RERANK_API_KEY=

# -----------------------------------------------------------------------------
# API PROXY (профиль: api, порт 8080)
# -----------------------------------------------------------------------------
# URL backend'а (автоматически определяется по LLM_BACKEND, если пусто)
LLM_BACKEND_URL=

# API ключ для прокси (опционально)
API_KEY=
